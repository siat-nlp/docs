{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"NLP & DL Tricks \u00b6 This repository aims to keep track of some practical and theoretical tricks in natural language processing (NLP) / deep learning / machine learning, etc. Most of these tricks are summarized by members of our group, while some others are borrowed from open-source sites. Data prepossessing \u00b6 Network architecture \u00b6 Seq2Seq \u00b6 Some tricks to train RNN and seq2seq models: Embedding size: 1024 or 512. Lower dimensionality like 256 can also lead to good performances. Higher does not necessarily lead to better performances. For the decoder: LSTM > GRU > Vanilla-RNN 2-4 layers seems generally enough. Deeper models with residual connections seems more difficult to converge (high variance). More tricks needs to be discovered. ResD (dense residual connections) > Res (only connected to previous layer) > no residual connections For encoder: Bidirectional > Unidirectional (reversed input) > Unidirectional Attention (additive) > Attention (multiplicative) > No attention. Authors suggest that attention act more as a skip connection mechanism than as a memory for the decoder. Ref : Massive Exploration of Neural Machine Translation Architectures , Denny Britz, Anna Goldie et al. For seq2seq, reverse the order of the input sequence (['I', 'am', 'hungry'] becomes ['hungry', 'am', 'I']). Keep the target sequence intact. Why : From the authors: \" This way, [...] that makes it easy for SGD to \u201cestablish communication\u201d between the input and the output. We found this simple data transformation to greatly improve the performance of the LSTM. \" Ref : Sequence to Sequence Learning with Neural Networks , Ilya Sutskever et al. Char-RNN \u00b6 By training in an unsupervised way a network to predict the next character of a text (char-RNN), the network will learn a representation which can then be used for a supervised task (here sentiment analysis). Ref : Learning to Generate Reviews and Discovering Sentiment , Ilya Sutskever et al. Parameters \u00b6 Learning rate \u00b6 The learning rate can be usually initialized as 0.0001, 0.0003, 0.001, 0.003, 0.01, 0.03, 0.1(3x growing up). A strategy used to select the hyperparameters is to randomly sample them (uniformly or logscale) and see the testing error after a few epoch. Beam size \u00b6 Usually set from 2 to 10. The larger beam size, the higher computational cost. Regularization \u00b6 Dropout \u00b6 To make Dropout works with RNN, it should only be applied on non-recurrent connections (between layers among a same timestep) [1]. Some more recent paper propose some tricks to make dropout works for recurrent connections[2]. Ref : [1]. Recurrent Neural Network Regularization , Wojciech Zaremba et al. [2]. Recurrent Dropout without Memory Loss , Stanislau Semeniuta et al. Batch normalization \u00b6 adding a new normalization layer. Some additional tricks for accelerating BN Networks: * Increase the learning rate * Remove/reduce Dropout: speeds up training, without increasing overfitting * Remove/Reduce the L2 weight regularization * Accelerate the learning rate decay: because the network trains faster * Remove Local Response Normalization * Shuffle training examples more thoroughly: prevents the same examples from always appearing in a mini-batch together. (The authors speak about 1% improvements in the validation) * Reduce the photometric distortions Why : Some good explanation at Quora . Reinforcement learning \u00b6 Asynchronous \u00b6 Train simultaneously multiple agents with different exploration policies (e.g., E-greedy with different values of epsilon) improve the robustness. Ref : Asynchronous Methods for Deep Reinforcement Learning , V. Mnih. Skip frame \u00b6 Compute the action every 4 frames instead of every frames. For the other frames, repeat the action. Why : Works well on Atari games, when the player reactivity doesn't need to be frame perfect. Using this trick allows to greatly speed up the training (About x4). Ref : Playing Atari with Deep Reinforcement Learning , V. Mnih. History \u00b6 Instead of only taking the current frame as input, stack the last frames together on a single input (size (h, w, c) with 1 grayscale frame by channel). Combined with a skip frame (repeat action) of 4, that means we would stack the frames t, t-4, t-8 and t-12. Why : This allows the network to have some momentum information. Ref : Deep Reinforcement Learning with Double Q-learning , V. Mnih. Experience Replay \u00b6 Instead of updating every frames as the agent plays, to avoid correlations between the frames, it's better to sample a batch in the history of the transition taken (state, actionTaken, reward, nextState). This is basically the same idea as shuffling the dataset before training for supervised tasks. Some strategies exist to sample batches which contain more information (in the sense predicted reward different from real reward). Ref : Prioritized Experience Replay , Tom Schaul et al. PAAC (Parallel Advantage Actor Critic) \u00b6 It's possible to simplify the the A3C algorithm by batching the agent experiences and using a single model with synchronous updates. Ref : Efficient Parallel Methods for Deep Reinforcement Learning , Alfredo V. Clemente et al.","title":"NLP & DL Tricks"},{"location":"#nlp-dl-tricks","text":"This repository aims to keep track of some practical and theoretical tricks in natural language processing (NLP) / deep learning / machine learning, etc. Most of these tricks are summarized by members of our group, while some others are borrowed from open-source sites.","title":"NLP &amp; DL Tricks"},{"location":"#data-prepossessing","text":"","title":"Data prepossessing"},{"location":"#network-architecture","text":"","title":"Network architecture"},{"location":"#seq2seq","text":"Some tricks to train RNN and seq2seq models: Embedding size: 1024 or 512. Lower dimensionality like 256 can also lead to good performances. Higher does not necessarily lead to better performances. For the decoder: LSTM > GRU > Vanilla-RNN 2-4 layers seems generally enough. Deeper models with residual connections seems more difficult to converge (high variance). More tricks needs to be discovered. ResD (dense residual connections) > Res (only connected to previous layer) > no residual connections For encoder: Bidirectional > Unidirectional (reversed input) > Unidirectional Attention (additive) > Attention (multiplicative) > No attention. Authors suggest that attention act more as a skip connection mechanism than as a memory for the decoder. Ref : Massive Exploration of Neural Machine Translation Architectures , Denny Britz, Anna Goldie et al. For seq2seq, reverse the order of the input sequence (['I', 'am', 'hungry'] becomes ['hungry', 'am', 'I']). Keep the target sequence intact. Why : From the authors: \" This way, [...] that makes it easy for SGD to \u201cestablish communication\u201d between the input and the output. We found this simple data transformation to greatly improve the performance of the LSTM. \" Ref : Sequence to Sequence Learning with Neural Networks , Ilya Sutskever et al.","title":"Seq2Seq"},{"location":"#char-rnn","text":"By training in an unsupervised way a network to predict the next character of a text (char-RNN), the network will learn a representation which can then be used for a supervised task (here sentiment analysis). Ref : Learning to Generate Reviews and Discovering Sentiment , Ilya Sutskever et al.","title":"Char-RNN"},{"location":"#parameters","text":"","title":"Parameters"},{"location":"#learning-rate","text":"The learning rate can be usually initialized as 0.0001, 0.0003, 0.001, 0.003, 0.01, 0.03, 0.1(3x growing up). A strategy used to select the hyperparameters is to randomly sample them (uniformly or logscale) and see the testing error after a few epoch.","title":"Learning rate"},{"location":"#beam-size","text":"Usually set from 2 to 10. The larger beam size, the higher computational cost.","title":"Beam size"},{"location":"#regularization","text":"","title":"Regularization"},{"location":"#dropout","text":"To make Dropout works with RNN, it should only be applied on non-recurrent connections (between layers among a same timestep) [1]. Some more recent paper propose some tricks to make dropout works for recurrent connections[2]. Ref : [1]. Recurrent Neural Network Regularization , Wojciech Zaremba et al. [2]. Recurrent Dropout without Memory Loss , Stanislau Semeniuta et al.","title":"Dropout"},{"location":"#batch-normalization","text":"adding a new normalization layer. Some additional tricks for accelerating BN Networks: * Increase the learning rate * Remove/reduce Dropout: speeds up training, without increasing overfitting * Remove/Reduce the L2 weight regularization * Accelerate the learning rate decay: because the network trains faster * Remove Local Response Normalization * Shuffle training examples more thoroughly: prevents the same examples from always appearing in a mini-batch together. (The authors speak about 1% improvements in the validation) * Reduce the photometric distortions Why : Some good explanation at Quora .","title":"Batch normalization"},{"location":"#reinforcement-learning","text":"","title":"Reinforcement learning"},{"location":"#asynchronous","text":"Train simultaneously multiple agents with different exploration policies (e.g., E-greedy with different values of epsilon) improve the robustness. Ref : Asynchronous Methods for Deep Reinforcement Learning , V. Mnih.","title":"Asynchronous"},{"location":"#skip-frame","text":"Compute the action every 4 frames instead of every frames. For the other frames, repeat the action. Why : Works well on Atari games, when the player reactivity doesn't need to be frame perfect. Using this trick allows to greatly speed up the training (About x4). Ref : Playing Atari with Deep Reinforcement Learning , V. Mnih.","title":"Skip frame"},{"location":"#history","text":"Instead of only taking the current frame as input, stack the last frames together on a single input (size (h, w, c) with 1 grayscale frame by channel). Combined with a skip frame (repeat action) of 4, that means we would stack the frames t, t-4, t-8 and t-12. Why : This allows the network to have some momentum information. Ref : Deep Reinforcement Learning with Double Q-learning , V. Mnih.","title":"History"},{"location":"#experience-replay","text":"Instead of updating every frames as the agent plays, to avoid correlations between the frames, it's better to sample a batch in the history of the transition taken (state, actionTaken, reward, nextState). This is basically the same idea as shuffling the dataset before training for supervised tasks. Some strategies exist to sample batches which contain more information (in the sense predicted reward different from real reward). Ref : Prioritized Experience Replay , Tom Schaul et al.","title":"Experience Replay"},{"location":"#paac-parallel-advantage-actor-critic","text":"It's possible to simplify the the A3C algorithm by batching the agent experiences and using a single model with synchronous updates. Ref : Efficient Parallel Methods for Deep Reinforcement Learning , Alfredo V. Clemente et al.","title":"PAAC (Parallel Advantage Actor Critic)"},{"location":"dlFramework/","text":"DL Framework Programming \u00b6 This repository aims to keep track of some practical and theoretical tricks in natural language processing (NLP) / deep learning / machine learning, etc. Most of these tricks are summarized by members of our group, while some others are borrowed from open-source sites. Programming in Tensorflow \u00b6 tf.variable_scope/tf.name_scope \u00b6 Both scopes have the same effect on all operations as well as variables, but name scope is ignored by tf.get_variable . Suggest use tf.variable_scope in most cases. Ref : The difference between name scope and variable scope in tensorflow at stackoverflow . Model Save/Restore \u00b6 Usually, we create a helper saver = tf.train.Saver() to save and restore the whole model. However, if we want to use pre-trained model for fine-tuning or transfer learning, there are 2 ways: (1) Create the network by writing code to create each and every layer manually as the original model, and then use tf.train.Saver() to restore pre-trained model's checkpoint file. (2) Use .meta file and create the helper as saver = tf.train.import_meta_graph('xxx_model-xxx.meta') and then restore the pre-trained model. Ref : More details are in this tutorial . Programming in PyTorch \u00b6","title":"DL Framework Programming"},{"location":"dlFramework/#dl-framework-programming","text":"This repository aims to keep track of some practical and theoretical tricks in natural language processing (NLP) / deep learning / machine learning, etc. Most of these tricks are summarized by members of our group, while some others are borrowed from open-source sites.","title":"DL Framework Programming"},{"location":"dlFramework/#programming-in-tensorflow","text":"","title":"Programming in Tensorflow"},{"location":"dlFramework/#tfvariable_scopetfname_scope","text":"Both scopes have the same effect on all operations as well as variables, but name scope is ignored by tf.get_variable . Suggest use tf.variable_scope in most cases. Ref : The difference between name scope and variable scope in tensorflow at stackoverflow .","title":"tf.variable_scope/tf.name_scope"},{"location":"dlFramework/#model-saverestore","text":"Usually, we create a helper saver = tf.train.Saver() to save and restore the whole model. However, if we want to use pre-trained model for fine-tuning or transfer learning, there are 2 ways: (1) Create the network by writing code to create each and every layer manually as the original model, and then use tf.train.Saver() to restore pre-trained model's checkpoint file. (2) Use .meta file and create the helper as saver = tf.train.import_meta_graph('xxx_model-xxx.meta') and then restore the pre-trained model. Ref : More details are in this tutorial .","title":"Model Save/Restore"},{"location":"dlFramework/#programming-in-pytorch","text":"","title":"Programming in PyTorch"},{"location":"tutorial/","text":"Useful Tutorials \u00b6 under construction...","title":"Useful Tutorials"},{"location":"tutorial/#useful-tutorials","text":"under construction...","title":"Useful Tutorials"},{"location":"helper/","text":"DL Framework Programming \u00b6 This repository aims to keep track of some practical and theoretical tricks in natural language processing (NLP) / deep learning / machine learning, etc. Most of these tricks are summarized by members of our group, while some others are borrowed from open-source sites. Programming in Tensorflow \u00b6 tf.variable_scope/tf.name_scope \u00b6 Both scopes have the same effect on all operations as well as variables, but name scope is ignored by tf.get_variable . Suggest use tf.variable_scope in most cases. Ref : The difference between name scope and variable scope in tensorflow at stackoverflow . Model Save/Restore \u00b6 Usually, we create a helper saver = tf.train.Saver() to save and restore the whole model. However, if we want to use pre-trained model for fine-tuning or transfer learning, there are 2 ways: (1) Create the network by writing code to create each and every layer manually as the original model, and then use tf.train.Saver() to restore pre-trained model's checkpoint file. (2) Use .meta file and create the helper as saver = tf.train.import_meta_graph('xxx_model-xxx.meta') and then restore the pre-trained model. Ref : More details are in this tutorial . Programming in PyTorch \u00b6","title":"How to contri?"},{"location":"helper/#dl-framework-programming","text":"This repository aims to keep track of some practical and theoretical tricks in natural language processing (NLP) / deep learning / machine learning, etc. Most of these tricks are summarized by members of our group, while some others are borrowed from open-source sites.","title":"DL Framework Programming"},{"location":"helper/#programming-in-tensorflow","text":"","title":"Programming in Tensorflow"},{"location":"helper/#tfvariable_scopetfname_scope","text":"Both scopes have the same effect on all operations as well as variables, but name scope is ignored by tf.get_variable . Suggest use tf.variable_scope in most cases. Ref : The difference between name scope and variable scope in tensorflow at stackoverflow .","title":"tf.variable_scope/tf.name_scope"},{"location":"helper/#model-saverestore","text":"Usually, we create a helper saver = tf.train.Saver() to save and restore the whole model. However, if we want to use pre-trained model for fine-tuning or transfer learning, there are 2 ways: (1) Create the network by writing code to create each and every layer manually as the original model, and then use tf.train.Saver() to restore pre-trained model's checkpoint file. (2) Use .meta file and create the helper as saver = tf.train.import_meta_graph('xxx_model-xxx.meta') and then restore the pre-trained model. Ref : More details are in this tutorial .","title":"Model Save/Restore"},{"location":"helper/#programming-in-pytorch","text":"","title":"Programming in PyTorch"}]}